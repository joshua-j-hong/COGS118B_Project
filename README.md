# Applying PCA and Clustering to Star Classification

## Introduction

Over the course of this quarter, we have explored a number of methods for identifying trends within unlabeled data. While techniques such as PCA aim to reduce the dimensionality of data sets while retaining as much variance as possible, clustering methods like Spectral Clustering and K-means group data points into categories with different measures of similarity. However, the examples given in this class consist of only continuous data, while data in the real world often comes in a variety of formats. Therefore, it becomes useful to have a more general understanding of how well these techniques behave on datasets with broad data types. 

This paper aims to assess the performance of PCA and a suite of clustering algorithms on datasets with mixed data types. We selected a novel dataset on star classification and attempted to apply PCA and clustering methods to attain clusters similar to the star classes given. 

## Related Work

Studies in the past have explored similar problems regarding non-continuous data and PCA. One such study (Kolenikov, 2004) discusses the usability of PCA on discrete data regarding socioeconomic indices, with the overall conclusion being that using PCA on a set of dummy variables performs worse than other methods. However, a recent paper (Logan, 2020) presents unsupervised learning as an alternative to supervised methods to separate stars, galaxies and quasi-stellar objects. Within this paper, they use HDBSCAN, an unsupervised learning algorithm for density based clustering, as well as Random Forest and PCA for feature selection and dimensionality reduction to achieve high F1 scores when compared to labelled data. While we see that unsupervised learning has been demonstrated to work on stellar data, we explore if the results of Kolenikov can be generalized to other datasets by analyzing our mixed dataset. 

## Dataset

The dataset chosen for this analysis is a star dataset with 6 classes. The dataset contains 240 data points and a mixture of data types, with the majority being continuous and a few being categorical. The classes are distributed equally, with 40 of the data points belonging to each class. For the two discrete variables, one has 7 well defined values while the other has a variety of values that need to be preprocessed before analysis. 

## Methods

As our methods can be roughly broken into two categories, we outline two sections, with the first concerning data preprocessing and the second concerning clustering algorithms. 

### Data Preprocessing

To begin, we clean our dataset to get it into a usable format. After checking for null values and removing the corresponding data points, we convert the “spectral class” and “star color” classes into binary variables. While the “spectral class” variable had a number of well defined categories, the “star color” variable required more work to condense and combine similar categories. 

Next, we apply PCA to the continuous variables and binary variables. To aid with our analysis later, we perform PCA twice; once with all variables in the dataset and again with only the continuous variables. After finding the principal component coordinates of our data points, we select enough principal components to capture over 90% of the variance of the original data.

### Clustering Algorithms 

Once we have expressed the data in principal component coordinates, we conduct clustering with the set of algorithms that we learned in the quarter, K-means, Mixture of Gaussians, and Spectral Clustering. Furthermore, we run spectral clustering with two different similarity functions: a Gaussian kernel and a graph of nearest neighbors. We run the clustering algorithms on the PCA results from the full data and the PCA results from the condensed dataset separately.
To compare the performance of the clustering algorithms used, we use the adjusted_rand_score method from the sklearn.metrics module. This metric calculates the Rand Index between two clusterings, which is a measurement of the similarity between the two clusters using sample pairs. Next, the Rand Index is adjusted for chance, which gives a metric that ranges from 0 if the data points are labeled randomly and 1 if the clusterings are almost identical. 

## Results

Setting out, our goal was to find the “Star type” groups using a series of unsupervised learning methods. As mentioned above, we utilized PCA to reduce our 18 features down to much fewer dimensions. We implemented a way of checking the amount of variance in the data that was explained by each principal component (using the eigenvalues) and noted the variance contribution of each principal component. We did a sanity check for the PCA explained variance of the dataset to ensure our implementation is functional, which was successful. Setting the threshold of cumulative explained variance to 0.9, we included the first nine (0-8 PC) principal components. 

Upon plotting out the 3D graph of the first three components, we did not see any clear clustering groups, and we became a little skeptical going forward about our ability to cluster the initial PCA result. Indeed, after running all three clustering methods, we found relatively low accuracy results (which we computed using the rand_score) and decided to look back to the PCA section to see what we could do differently. Then we did more research, and an article by Brandon Walker in Towards Data Science presented us with an apparent reason for our poor result: PCA does not work well with categorical data. After locating the issue, we decided to exclude the categorical data and represent the data points with only four continuous variables. 

At this point, we wondered whether PCA would be necessary to achieve our desired clustering results, considering that our data points are already living in such a low dimensionality space. In order to demonstrate the necessity of PCA, we simply ran the three clustering algorithms on the condensed data and assessed the results (using the same method as above). Even with our most effective technique (which will be spoken about in detail in the sections below), we found that simply clustering the four-dimensional data without running PCA achieved very poor results (approx. 40% accuracy). Additionally, the Spectral Clustering algorithm (using the Gaussian kernel) that we had hoped to utilize would only distinguish 6 distinct clusters about a third of the time it was run. At this point, we hoped that re-representing the data with PCA once again would allow the clusters to be more readily separated. After conducting PCA and visualizing the first three principal component coordinates with a 3D graph, we were confident that our clustering algorithms could be more successful. For consistency, we used the first three PC’s, as they explained 90% of the variance. Here, we repeated our clustering methods accordingly for K means, MOG, and Spectral Clustering. While the accuracy was greatly improved from the previous attempts at clustering (see in chart below), we were not satisfied with these results and hoped that we could adjust the Spectral Clustering algorithm to better suit the data. 

Up to now, we had been using the default parameters for all three algorithms; K-Means and MOG do not have much flexibility, but as we discussed in class, the Spectral Clustering algorithm has a very important hyperparameter that represents the variance of the Gaussian kernel (used to construct the affinity matrix). In the sklearn method that we used to implement Spectral Clustering, this hyperparameter is called gamma. After adjusting the gamma value across a range (between .5 and 5), we landed on ~3 as the optimal value. The results generated from this adjustment, however, still did not match the accuracy of those from the MOG algorithm. While searching the Spectral Clustering documentation on sklearn, we came across another method of generating the affinity matrix called nearest neighbors; this modified Spectral Clustering algorithm relies on the mutual nearest neighbors of data points to characterize their affinity. By observing the 2D plot of the first two principal components, it appeared that the nearest neighbors formulation would suit the data better than the Gaussian kernel. After re-running the Spectral Clustering algorithm with the nearest neighbors affinity matrix, it immediately showed itself to be the best algorithm for distinguishing the groups in the data. To achieve the optimal result of the algorithm, we merely needed to test a range of n_neighbors values.  In the end, we got our best clustering result using 18 nearest neighbors. 

## Discussion

From this project, one thing we have learned is that PCA does not work well on the categorical/discrete data, and this is the reason why the outcome of clustering after PCA on only the four continuous variables is significantly better than that on the whole dataset. Intuitively, conducting PCA on the whole dataset should capture more variance, and thus, the outcome should be better. However, since PCA transforms data from one coordinate system to another, it is wrong to use it to transform categorical data because categorical data is not on a coordinate system. If we had more time, we would do more research on how to reduce the dimensionality of our dataset with the categorical variables, because “color” and “spectral class” might contain important information that we cannot discard. Additionally, we could research deeper about “color” and “spectral class” to see if the information provided by these variables can be represented as continuous variables. 

Another thing we learned was that without normalization of the data, PCA returns values that do not cluster as well as if the data were normalized. In the lecture slides, data was not normalized and was only zero-meaned before using PCA. However, when we did this, our clustering methods did not return very good results. When plotting different principal component pairs against one another when data was not normalized beforehand, we were unable to easily distinguish clusters in the graphs. Clusters tended to be interwoven within one another, making it difficult to separate clusters. With normalized data however, the separate clusters were able to be distinguished more easily than without normalized data. The most important result that came from normalizing data was with our clustering methods. When using non-normalized data after PCA with our clustering methods, we had really bad results where most data points were assigned to one singular cluster and maybe one or two points assigned to another cluster. This means that with non-normalized data, half our clusters ended up not even being returned by the clustering methods. With normalized data however, our clustering methods were able to return all 6 clusters, with our highest accuracy being 91.13%

In the future, using a larger dataset would be beneficial since our current dataset only has 240 datapoints, which left big holes in the distributions. However, since clustering techniques may not work well with large datasets, we could also use different learning methods in the future. For example, we could use supervised learning methods to predict star types and compare the results with the results from our unsupervised learning methods to see which generated the most accurate predictions. 

Link to paper: https://docs.google.com/document/d/1iMh9MrlYPaCcsqhdpG2dUkeXdABr_Qr2dswSvLav9CI/edit?usp=sharing

Link to video: https://www.youtube.com/watch?v=7_IlGjqNkmk
